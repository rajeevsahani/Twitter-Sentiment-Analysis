{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " 'has',\n",
       " 'hasn',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " 'she',\n",
       " 'should',\n",
       " 'shouldn',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " 'wouldn',\n",
       " 'y',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a field  \\n       of computer science, artificial intelligence  \\n       and computational linguistics concerned with  \\n       the interactions between computers and human  \\n       (natural) languages, and, in particular, \\n       concerned with programming computers to \\n       fruitfully process large natural language   \\n       corpora.', 'Challenges in natural language  \\n       processing frequently involve natural   \\n       language understanding, natural language  \\n       generation frequently from formal, machine  \\n       -readable logical forms), connecting language  \\n       and machine perception, managing human-\\n       computer dialog systems, or some combination\\n       thereof.']\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'language', 'generation', 'frequently', 'from', 'formal', ',', 'machine', '-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-', 'computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.']\n"
     ]
    }
   ],
   "source": [
    "# import the existing word and sentence tokenizing  \n",
    "# libraries \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "text = '''Natural language processing (NLP) is a field  \n",
    "       of computer science, artificial intelligence  \n",
    "       and computational linguistics concerned with  \n",
    "       the interactions between computers and human  \n",
    "       (natural) languages, and, in particular, \n",
    "       concerned with programming computers to \n",
    "       fruitfully process large natural language   \n",
    "       corpora. Challenges in natural language  \n",
    "       processing frequently involve natural   \n",
    "       language understanding, natural language  \n",
    "       generation frequently from formal, machine  \n",
    "       -readable logical forms), connecting language  \n",
    "       and machine perception, managing human-\n",
    "       computer dialog systems, or some combination\n",
    "       thereof.'''\n",
    "print(sent_tokenize(text)) \n",
    "print(word_tokenize(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(example_sent) \n",
    "  \n",
    "print(word_tokens)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "filtered_sentence = [] \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "#word_tokenize accepts a string as an input, not a file. \n",
    "stop_words = set(stopwords.words('english')) \n",
    "file1 = open(\"text.txt\") \n",
    "line = file1.read()# Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('filteredtext.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "programs  :  program\n",
      "programer  :  program\n",
      "programing  :  program\n",
      "programers  :  program\n"
     ]
    }
   ],
   "source": [
    "# import these modules \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "   \n",
    "ps = PorterStemmer()   \n",
    "# choose some words to be stemmed \n",
    "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programers  :  program\n",
      "program  :  program\n",
      "with  :  with\n",
      "programing  :  program\n",
      "languages  :  languag\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# importing modules \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "   \n",
    "ps = PorterStemmer()    \n",
    "sentence = \"Programers program with programing languages\"\n",
    "words = word_tokenize(sentence) \n",
    "   \n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "# import these modules \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sukanya', 'NNP'), (',', ','), ('Rajib', 'NNP'), ('Naba', 'NNP'), ('good', 'JJ'), ('friends', 'NNS'), ('.', '.')]\n",
      "[('Sukanya', 'NNP'), ('getting', 'VBG'), ('married', 'VBN'), ('next', 'JJ'), ('year', 'NN'), ('.', '.')]\n",
      "[('Marriage', 'NN'), ('big', 'JJ'), ('step', 'NN'), ('one', 'CD'), ('’', 'NNP'), ('life', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('exciting', 'VBG'), ('frightening', 'VBG'), ('.', '.')]\n",
      "[('But', 'CC'), ('friendship', 'NN'), ('sacred', 'VBD'), ('bond', 'NN'), ('people', 'NNS'), ('.', '.')]\n",
      "[('It', 'PRP'), ('special', 'JJ'), ('kind', 'NN'), ('love', 'VB'), ('us', 'PRP'), ('.', '.')]\n",
      "[('Many', 'JJ'), ('must', 'MD'), ('tried', 'VB'), ('searching', 'VBG'), ('friend', 'NN'), ('never', 'RB'), ('found', 'VBD'), ('right', 'JJ'), ('one', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "#Dummy text \n",
    "txt = \"\"\"Sukanya, Rajib and Naba are my good friends. \n",
    "    Sukanya is getting married next year.\n",
    "    Marriage is a big step in one’s life.\n",
    "    It is both exciting and frightening.\n",
    "    But friendship is a sacred bond between people.\n",
    "    It is a special kind of love between us.\n",
    "    Many of you must have tried searching for a friend \n",
    "    but never found the right one.\"\"\"\n",
    "  \n",
    "# sent_tokenize is one of instances of  \n",
    "# PunktSentenceTokenizer from the nltk.tokenize.punkt module   \n",
    "tokenized = sent_tokenize(txt) \n",
    "for i in tokenized:  \n",
    "    # Word tokenizers is used to find the words  \n",
    "    # and punctuation in a string \n",
    "    wordsList = nltk.word_tokenize(i)   \n",
    "    # removing stop words from wordList \n",
    "    wordsList = [w for w in wordsList if not w in stop_words]    \n",
    "    #  Using a Tagger. Which is part-of-speech  \n",
    "    # tagger or POS-tagger.  \n",
    "    tagged = nltk.pos_tag(wordsList)   \n",
    "    print(tagged) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Geeksfor', 'Geeks..', '.$$&* \\nis', ' for geeks']\n"
     ]
    }
   ],
   "source": [
    "# import TabTokenizer() method from nltk \n",
    "from nltk.tokenize import TabTokenizer    \n",
    "# Create a reference variable for Class TabTokenizer \n",
    "tk = TabTokenizer() \n",
    "# Create a string input \n",
    "gfg = \"Geeksfor\\tGeeks..\\t.$$&* \\nis\\t for geeks\"    \n",
    "# Use tokenize method \n",
    "geek = tk.tokenize(gfg)     \n",
    "print(geek) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 4 1 3 0 6 2 7 4 1 3 6]\n",
      "road  -  0.42471718586982765\n",
      "on  -  0.30218977576862155\n",
      "driven  -  0.30218977576862155\n",
      "is  -  0.30218977576862155\n",
      "car  -  0.42471718586982765\n",
      "the  -  0.6043795515372431\n",
      "highway  -  0.0\n",
      "truck  -  0.0\n",
      "on  -  0.30218977576862155\n",
      "driven  -  0.30218977576862155\n",
      "is  -  0.30218977576862155\n",
      "the  -  0.6043795515372431\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "docA = \"The car is driven on the road\"\n",
    "docB = \"The truck is driven on the highway\"\n",
    "tfidf = TfidfVectorizer()\n",
    "response = tfidf.fit_transform([docA, docB])\n",
    "#print(response)\n",
    "feature_names = tfidf.get_feature_names()\n",
    "#print(feature_names)\n",
    "print(response.nonzero()[1])\n",
    "for col in response.nonzero()[1]:\n",
    "    print (feature_names[col], ' - ', response[0, col])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Geeksfor', 'Geeks..', '.$$&* \\nis', ' for geeks']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import TabTokenizer() method from nltk \n",
    "from nltk.tokenize import TabTokenizer \n",
    "     \n",
    "# Create a reference variable for Class TabTokenizer \n",
    "tk = TabTokenizer() \n",
    "     \n",
    "# Create a string input \n",
    "gfg = \"Geeksfor\\tGeeks..\\t.$$&* \\nis\\t for geeks\"\n",
    "     \n",
    "# Use tokenize method \n",
    "geek = tk.tokenize(gfg) \n",
    "     \n",
    "print(geek) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The price', ' of burger ', 'in BurgerKing is Rs.36.\\n']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import TabTokenizer() method from nltk \n",
    "from nltk.tokenize import TabTokenizer \n",
    "     \n",
    "# Create a reference variable for Class TabTokenizer \n",
    "tk = TabTokenizer() \n",
    "     \n",
    "# Create a string input \n",
    "gfg = \"The price\\t of burger \\tin BurgerKing is Rs.36.\\n\"\n",
    "     \n",
    "# Use tokenize method \n",
    "geek = tk.tokenize(gfg) \n",
    "     \n",
    "print(geek) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n",
      "plan\n",
      "a series of steps to be carried out or goals to be accomplished\n",
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "# First, you're going to need to import wordnet: \n",
    "from nltk.corpus import wordnet \n",
    "# Then, we're going to use the term \"program\" to find synsets like so: \n",
    "syns = wordnet.synsets(\"program\")  \n",
    "# An example of a synset: \n",
    "print(syns[0].name())   \n",
    "# Just the word: \n",
    "print(syns[0].lemmas()[0].name())   \n",
    "# Definition of that first synset: \n",
    "print(syns[0].definition())   \n",
    "# Examples of the word in use in sentences: \n",
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'honest', 'thoroughly', 'in_effect', 'commodity', 'practiced', 'serious', 'dear', 'near', 'in_force', 'well', 'skilful', 'honorable', 'undecomposed', 'dependable', 'goodness', 'proficient', 'salutary', 'ripe', 'trade_good', 'estimable', 'sound', 'adept', 'right', 'secure', 'unspoilt', 'safe', 'upright', 'effective', 'soundly', 'full', 'good', 'respectable', 'just', 'expert', 'skillful', 'beneficial', 'unspoiled'}\n",
      "{'evilness', 'evil', 'ill', 'bad', 'badness'}\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "synonyms = [] \n",
    "antonyms = [] \n",
    "  \n",
    "for syn in wordnet.synsets(\"good\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name())   \n",
    "print(set(synonyms)) \n",
    "print(set(antonyms)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
